{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc844a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RDD Approach:\n",
      "Age Group: 25-64, Count: 121093\n",
      "Age Group: 18-24, Count: 33605\n",
      "Age Group: <18, Count: 15928\n",
      "Age Group: >64, Count: 5985\n",
      "Total execution time (RDD): 25.94 seconds\n",
      "\n",
      "DataFrame Approach:\n",
      "Age Group: 25-64, Count: 121093\n",
      "Age Group: 18-24, Count: 33605\n",
      "Age Group: <18, Count: 15928\n",
      "Age Group: >64, Count: 5985\n",
      "Total execution time (DataFrame): 9.38 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import col, when\n",
    "import time\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def get_age_group(age):\n",
    "    if age < 18:\n",
    "        return \"<18\"\n",
    "    elif 18 <= age <= 24:\n",
    "        return \"18-24\"\n",
    "    elif 25 <= age <= 64:\n",
    "        return \"25-64\"\n",
    "    elif age > 64:\n",
    "        return \">64\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_data_with_rdd(s3_link1, s3_link2):\n",
    "    conf = SparkConf().setAppName(\"IncidentsPerAgeGroupRDD\").setMaster(\"yarn\").set(\"spark.executor.instances\", \"4\")\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "    def read_file(s3_link):\n",
    "        data = sc.textFile(s3_link)\n",
    "        header = data.first()\n",
    "        rows = data.filter(lambda row: row != header).map(lambda line: list(csv.reader([line]))[0])\n",
    "        return rows\n",
    "\n",
    "    combined_data = read_file(s3_link1).union(read_file(s3_link2))\n",
    "    aggravated_assault = combined_data.filter(lambda row: \"AGGRAVATED ASSAULT\" in row[9].strip().upper())\n",
    "    age_groups = aggravated_assault.map(lambda row: (get_age_group(int(row[11]) if row[11].isdigit() else -1), 1)).filter(lambda x: x[0] is not None)\n",
    "    age_group_counts = age_groups.reduceByKey(lambda a, b: a + b)\n",
    "    sorted_results = age_group_counts.sortBy(lambda x: x[1], ascending=False).collect()\n",
    "    result_dict = dict(sorted_results)\n",
    "    return result_dict\n",
    "\n",
    "def process_data_with_dataframe(s3_link1, s3_link2):\n",
    "    conf = SparkConf().setAppName(\"IncidentsPerAgeGroupDataFrame\").setMaster(\"yarn\").set(\"spark.executor.instances\", \"4\")\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "    \n",
    "    def read_file(s3_link):\n",
    "        df = spark.read.csv(s3_link, header=True, inferSchema=True)\n",
    "        return df\n",
    "\n",
    "    df = read_file(s3_link1).union(read_file(s3_link2))\n",
    "    df = df.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "    df = df.withColumn(\"Age Group\", when(col(\"Vict Age\") < 18, \"<18\")\n",
    "                                    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"18-24\")\n",
    "                                    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"25-64\")\n",
    "                                    .when(col(\"Vict Age\") > 64, \">64\")\n",
    "                                    .otherwise(None))\n",
    "    df = df.groupBy(\"Age Group\").count()\n",
    "    df = df.orderBy(col(\"count\").desc())\n",
    "    result_dict = {row['Age Group']: row['count'] for row in df.collect()}\n",
    "    return result_dict\n",
    "\n",
    "def print_results(results, approach_name):\n",
    "    print(f\"\\n{approach_name} Approach:\")\n",
    "    for group, count in results.items():\n",
    "        print(f\"Age Group: {group}, Count: {count}\")\n",
    "\n",
    "s3_link1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "s3_link2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "start_time_rdd = time.time()\n",
    "results_rdd = process_data_with_rdd(s3_link1, s3_link2)\n",
    "end_time_rdd = time.time()\n",
    "execution_time_rdd = end_time_rdd - start_time_rdd\n",
    "print_results(results_rdd, \"RDD\")\n",
    "print(f\"Total execution time (RDD): {execution_time_rdd:.2f} seconds\")\n",
    "\n",
    "start_time_df = time.time()\n",
    "results_df = process_data_with_dataframe(s3_link1, s3_link2)\n",
    "end_time_df = time.time()\n",
    "execution_time_df = end_time_df - start_time_df\n",
    "print_results(results_df, \"DataFrame\")\n",
    "print(f\"Total execution time (DataFrame): {execution_time_df:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
